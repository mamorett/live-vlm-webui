# Live VLM WebUI

**A universal web interface for real-time Vision Language Model interaction and benchmarking.**

Stream your webcam to any VLM and get live AI-powered analysis - perfect for testing models, benchmarking performance, and exploring vision AI capabilities across different hardware platforms.

## Key Highlights

üåê **Universal Compatibility** - Works with **any VLM served via OpenAI-compatible API** using base64-encoded images. Deploy on:
- üü¢ **NVIDIA Jetson** (Orin, AGX Xavier)
- üîµ **NVIDIA DGX Spark** systems
- üñ•Ô∏è **Desktop/Workstation** (Linux, potentially Mac)
- ‚òÅÔ∏è **Cloud APIs** (OpenAI, Anthropic, etc.)

## Features

### Core Functionality
- üé• **Real-time WebRTC streaming** - Low-latency bidirectional video
- üîå **OpenAI-compatible API** - Works with vLLM, SGLang, Ollama, TGI, or any vision API endpoint that uses base64-encoded images
- üìù **Interactive prompt editor** - 10+ preset prompts (scene description, object detection, safety monitoring, OCR, etc.) + custom prompts
- ‚ö° **Async processing** - Smooth video while VLM processes frames in background
- üîß **Flexible deployment** - Local inference or cloud APIs (OpenAI, Anthropic, etc.)

### UI & Visualization
- üé® **Modern NVIDIA-themed UI** - Professional design inspired by NVIDIA NGC Catalog with NVIDIA green accents
- üåì **Light/Dark theme toggle** - Automatic preference persistence with localStorage
- üìä **Live system monitoring** - Real-time GPU, VRAM, CPU, and RAM stats with sparkline charts
- ‚è±Ô∏è **Inference metrics** - Live latency tracking (last, average, total count) displayed with VLM output
- ü™û **Video mirroring** - Toggle button overlay on camera view
- üì± **Compact layout** - Single-screen design with all content visible (video, output, stats, controls)

### Configuration & Control
- üéõÔ∏è **Dynamic settings** - Change model, prompt, and processing interval without restarting
- üìπ **Multi-camera support** - Detect and switch between multiple cameras (USB webcams, built-in cameras)
- üîÑ **Model auto-detection** - Refresh button to discover available models from API
- ‚öôÔ∏è **Adjustable processing rate** - Control frame interval (1-3600 frames, default 30)
- üéØ **Max tokens control** - Fine-tune output length (1-4096 tokens)
- üîå **WebSocket real-time updates** - Instant feedback on settings and analysis results

### Platform Support
- üíª **Cross-platform monitoring** - Auto-detects NVIDIA GPUs (NVML), with framework for Apple Silicon and AMD
- üñ•Ô∏è **Dynamic system detection** - Automatically displays CPU model name and hostname
  - Linux: reads from `/proc/cpuinfo`
  - macOS: uses `sysctl machdep.cpu.brand_string`
  - Windows: uses WMIC
- üîí **HTTPS support** - Self-signed certificates for secure webcam access

## Future Enhancements (Roadmap)

- [ ] Benchmark mode for side-by-side model comparison
- [ ] Model download UI (‚ûï button placeholder ready)
- [ ] Cloud API templates (OpenAI, Anthropic quick configs)
- [ ] Recording functionality (save analysis sessions)
- [ ] Export results (JSON, CSV)
- [ ] Mobile app support
- [ ] **Hardware-accelerated video processing on Jetson** - Use NVENC/NVDEC for color space conversion instead of CPU-based swscaler (current implementation uses software conversion which is not optimal for Jetson platforms)

## Screenshot

![](./docs/images/chrome_app_running.png)

## Architecture

1. **Uplink**: Webcam video ‚Üí WebRTC ‚Üí Server
2. **Processing**: Server extracts frames ‚Üí VLM analyzes based on your prompt (async)
3. **Downlink**: Clean video stream ‚Üí WebRTC ‚Üí Browser
4. **UI Updates**: VLM results ‚Üí WebSocket ‚Üí Real-time text display

The VLM processes frames asynchronously in the background. The video stream continues smoothly while results are displayed in a separate text panel via WebSocket, ensuring no visual interference and instant updates when new analysis completes.

## Prerequisites

- Python 3.8+
- A VLM serving backend (choose one):
  - [vLLM](https://github.com/vllm-project/vllm) (recommended for performance)
  - [SGLang](https://github.com/sgl-project/sglang) (good for complex reasoning)
  - [Ollama](https://ollama.ai/) (easiest to get started)
  - Any OpenAI-compatible API
- Webcam (V4L2 compliant video source)

## Installation

1. **Clone the repository**:
```bash
git clone https://github.com/nvidia-ai-iot/live-vlm-webui.git
cd live-vlm-webui
```

2. **Create a virtual environment**:

**Option A: venv (recommended for Jetson/lightweight systems):**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

**Option B: conda (recommended for desktop/workstation):**
```bash
conda create -n live-vlm-webui python=3.10 -y
conda activate live-vlm-webui
```

3. **Install Python dependencies**:
```bash
pip install -r requirements.txt
```

4. **Generate SSL certificates** (required for webcam access):
```bash
./generate_cert.sh
```

This will create `cert.pem` and `key.pem` in the project directory. These are self-signed certificates for local development.

**Note:** Modern browsers require HTTPS to access webcam/microphone. The self-signed certificate will trigger a security warning - you'll need to click "Advanced" ‚Üí "Proceed" to accept it.

### Platform-Specific Notes

**Jetson (ARM64):**
- Use `venv` instead of conda (lighter footprint)
- Recommended for Jetson Orin, AGX Xavier, Nano
```bash
python3 -m venv .venv
source .venv/bin/activate
```

**Desktop/Workstation (x86_64):**
- Either `venv` or `conda` works well
- `conda` better for data science workflows
- `venv` faster and more lightweight

**Why `.venv` (with dot)?**
- Modern Python standard (PEP 405)
- Auto-detected by VS Code, PyCharm
- Keeps directory clean (hidden by default)
- Already in `.gitignore`

5. **Set up your VLM backend** (choose one):

### Option A: Ollama (Easiest)
```bash
# Install ollama from https://ollama.ai/download
# Pull a vision model
ollama pull llava:7b

# Start ollama server
ollama serve
```

### Option B: vLLM (Recommended)
```bash
# Install vLLM
pip install vllm

# Start vLLM server with a vision model
python -m vllm.entrypoints.openai.api_server \
  --model llama-3.2-11b-vision-instruct \
  --port 8000
```

### Option B2: vLLM on Jetson Thor (Container)

For NVIDIA Jetson Thor, use the official vLLM container from NGC:

```bash
# Pull and run vLLM container with model cache mounted
docker run --rm -it \
  --network host \
  --shm-size=16g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  --runtime=nvidia \
  --name=vllm \
  -v $HOME/data/models/huggingface:/root/.cache/huggingface \
  -v $HOME/data/vllm_cache:/root/.cache/vllm \
  nvcr.io/nvidia/vllm:25.10-py3

# Inside the container, serve a vision model:
vllm serve microsoft/Phi-3.5-vision-instruct --trust-remote-code
```

**Notes:**
- Model files are cached in `$HOME/data/models/huggingface` for reuse
- First run will download the model (may take some time depending on model size)
- vLLM will be accessible at `http://localhost:8000` (default port)
- Use `--port` flag to change the port if needed
- For Phi-3.5-vision, approximately 8GB VRAM required

**Other recommended vision models for Jetson Thor:**
```bash
# Smaller, faster option (4B parameters)
vllm serve microsoft/Phi-3.5-vision-instruct --trust-remote-code

# Llama 3.2 Vision (11B parameters, higher quality)
vllm serve meta-llama/Llama-3.2-11B-Vision-Instruct --trust-remote-code

# Qwen2-VL (7B parameters, good balance)
vllm serve Qwen/Qwen2-VL-7B-Instruct --trust-remote-code
```

### Option C: SGLang
```bash
# Install SGLang
pip install "sglang[all]"

# Start SGLang server
python -m sglang.launch_server \
  --model-path llama-3.2-11b-vision-instruct \
  --port 30000
```

### Option D: NVIDIA API Catalog (Cloud)

Use NVIDIA's hosted API for instant access to vision models without local setup:

**1. Get your API Key:**
- Visit [NVIDIA API Catalog](https://build.nvidia.com/)
- Sign in with your NVIDIA account (free)
- Navigate to a vision model (e.g., [Llama 3.2 Vision](https://build.nvidia.com/meta/llama-3.2-90b-vision-instruct))
- Click "Get API Key" to generate your key

**2. Test with curl:**
```bash
# Example: Query Llama 3.2 Vision with an image
curl -X POST "https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-90b-vision-instruct/chat/completions" \
  -H "Authorization: Bearer nvapi-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxXYZ" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta/llama-3.2-90b-vision-instruct",
    "messages": [
      {
        "role": "user",
        "content": "What is in this image? <img src=\"data:image/png;base64,iVBORw0K...\" />"
      }
    ],
    "max_tokens": 512
  }'
```

**3. Use with live-vlm-webui:**

No server setup needed! Just configure via the web UI:
- **API Base URL**: `https://ai.api.nvidia.com/v1/gr`
- **API Key**: `nvapi-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxXYZ` (your actual key)
- **Model**: Select from available vision models (e.g., `meta/llama-3.2-90b-vision-instruct`)

The WebUI will auto-detect NVIDIA API Catalog and show/hide the API Key field accordingly.

**Available Vision Models:**
- `meta/llama-3.2-90b-vision-instruct` - Highest quality, 90B parameters
- `meta/llama-3.2-11b-vision-instruct` - Good balance, 11B parameters
- `microsoft/phi-3-vision-128k-instruct` - Fast and efficient, 4.2B parameters
- `nvidia/neva-22b` - NVIDIA's vision model, 22B parameters

**Notes:**
- Free tier available with rate limits
- No local GPU required
- Lowest latency for users without local VLM infrastructure
- API key format: `nvapi-` followed by ~60 alphanumeric characters
- Keep your API key secure - don't commit to git!

## Usage

### Quick Start

1. **Start your VLM backend** (see installation above)

2. **Start the server** (easiest way):
```bash
./start_server.sh
```

This will automatically start the server with SSL enabled using Ollama's `llama3.2-vision:11b` model.

3. **Open your browser** and navigate to:
```
https://<IP_ADDRESS>:8080
```

4. **Accept the security warning** (click "Advanced" ‚Üí "Proceed")

Click "**Advanced**" button.

![](./docs/images/chrome_advanced.png)

Then click on "**Proceeed to <IP_ADDRESS> (unsafe)**".

![](./docs/images/chrome_proceed.png)

5. **Click "OPEN CAMERA AND START VLM ANALYSIS"** and allow camera access

![](./docs/images/chrome_webcam_access.png)

### Using the Web Interface

Once the server is running, the web interface provides full control:

**Left Sidebar Controls:**
- **VLM API Configuration** - Change API URL, key, and model on-the-fly
  - **Model Selection** with compact icon buttons: üîÑ Refresh models | ‚ûï Download (coming soon)
  - Auto-detects available models from your VLM API
- **Camera and App Control** - Select camera and control application
  - Dropdown menu lists all detected video input devices
  - Switch cameras on-the-fly without restarting analysis
  - **Start/Stop buttons** - Open camera and start VLM analysis | Stop
- **Prompt Editor** - Choose from 10+ presets or write custom prompts
  - Adjust **Max Tokens** for response length (1-4096)
  - Presets include: scene description, object detection, safety monitoring, OCR, emotion detection, etc.
- **Processing Settings** - Set frame interval (1-3600 frames)
  - Lower (5-30) = more frequent analysis, higher GPU usage
  - Higher (60-300) = less frequent, good for benchmarking and power saving

**Main Content Area:**
- **Video Feed** - Live webcam with mirror toggle button (üîÑ) overlay in corner
- **VLM Output Card** - Real-time analysis results with:
  - Model name and inference latency metrics (last, average, count)
  - Current prompt display (shown in gray box with green accent)
  - Generated text output with fade effect to indicate freshness
- **System Stats Card** - Live monitoring with:
  - System info header: `hostname (CPU model) with GPU name`
  - GPU utilization and VRAM usage with progress bars
  - CPU and RAM stats with progress bars
  - Sparkline graphs showing historical trends (60-second window)

**Header:**
- **Logo and Title** - üé• "Live VLM WebUI" with subtitle
- **Connection Status** - Shows WebSocket connectivity (Connected/Disconnected)
- **Theme Toggle** - Switch between Light/Dark modes (üåô/‚òÄÔ∏è)

### Manual Usage

**With vLLM**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem
```

**With SGLang**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:30000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem
```

**With Ollama**:
```bash
python server.py --model llava:7b \
  --api-base http://localhost:11434/v1 \
  --ssl-cert cert.pem --ssl-key key.pem
```

### Custom Prompts - Beyond Captioning

The real power is in custom prompts! Here are some examples:

**Scene Description (default)**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --prompt "Describe what you see in this image in one sentence."
```

**Object Detection**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --prompt "List all objects you can see in this image."
```

**Safety Monitoring**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --prompt "Alert me if you see any safety hazards or dangerous situations."
```

**Activity Recognition**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --prompt "What activity is the person performing?"
```

**Accessibility**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --prompt "Describe the scene in detail for a visually impaired person."
```

**Emotion/Expression Detection**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --prompt "Describe the facial expressions and emotions you observe."
```

## Configuration

### Command-line Options

```bash
python server.py --help
```

**Required**:
- `--model MODEL` - VLM model name (e.g., `llama-3.2-11b-vision-instruct`)

**Optional**:
- `--host HOST` - Host to bind to (default: `0.0.0.0`)
- `--port PORT` - Port to bind to (default: `8080`)
- `--api-base URL` - VLM API base URL (default: `http://localhost:8000/v1`)
- `--api-key KEY` - API key, use `EMPTY` for local servers (default: `EMPTY`)
- `--prompt TEXT` - Custom prompt for VLM (default: scene description)
- `--process-every N` - Process every Nth frame (default: `30`)

### Example Configurations

**High-frequency updates** (more responsive, higher CPU usage):
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --process-every 15
```

**Custom port and host**:
```bash
python server.py --model llava:7b \
  --api-base http://localhost:11434/v1 \
  --host 0.0.0.0 \
  --port 3000
```

**Using OpenAI API** (or any remote service):
```bash
python server.py --model gpt-4-vision-preview \
  --api-base https://api.openai.com/v1 \
  --api-key your-api-key-here
```

## Performance Tuning

### Frame Processing Rate
You can adjust frame processing in two ways:

**Via Command Line** (at startup):
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --process-every 60  # Process every 60 frames
```

**Via Web UI** (while running):
- Go to "Processing Settings" in the left sidebar
- Change "Frame Processing Interval" (1-3600 frames)
- Click "Apply Settings" - takes effect immediately!

**Guidelines:**
- **Lower values** (5-15 frames) = more frequent analysis, higher GPU usage (~2-6 FPS @ 30fps)
- **Default** (30 frames) = balanced, ~1 FPS analysis @ 30fps video
- **Higher values** (60-120 frames) = less frequent, good for monitoring (~0.25-0.5 FPS)
- **Very high** (300-3600 frames) = infrequent updates for benchmarking or power saving
  - 300 frames = ~10 second intervals @ 30fps
  - 900 frames = ~30 second intervals @ 30fps
  - 3600 frames = ~2 minute intervals @ 30fps

### Model Selection
Choose based on your hardware and needs:

**Fast models (good for prototyping)**:
- `llava:7b` (Ollama)
- `llava-1.5-7b-hf` (vLLM/SGLang)

**Balanced**:
- `llama-3.2-11b-vision-instruct` (recommended)
- `llava:13b`

**High quality** (requires significant GPU memory):
- `llava:34b`
- `gpt-4-vision-preview` (via OpenAI API)

### Video Resolution
Edit `index.html` to change the requested video resolution:
```javascript
video: {
    width: { ideal: 640 },   // Lower for better performance
    height: { ideal: 480 }
}
```

## API Compatibility

This tool uses the OpenAI chat completions API format with vision support. Any backend that implements this standard will work:

### Tested Backends
- ‚úÖ **vLLM** - Best performance, production-ready
- ‚úÖ **SGLang** - Great for complex prompts
- ‚úÖ **Ollama** - Easiest setup
- ‚úÖ **OpenAI API** - Cloud-based (requires API key)

### Message Format
```python
{
  "model": "model-name",
  "messages": [{
    "role": "user",
    "content": [
      {"type": "text", "text": "your prompt"},
      {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}
    ]
  }]
}
```

## Use Cases

- üé¨ **Content Creation** - Live scene analysis for video production
- üîí **Security** - Real-time monitoring and alert generation
- ‚ôø **Accessibility** - Visual assistance for visually impaired users
- üéÆ **Gaming** - AI game master or interactive experiences
- üè• **Healthcare** - Activity monitoring, fall detection
- üè≠ **Industrial** - Quality control, safety monitoring
- üìö **Education** - Interactive learning experiences
- ü§ñ **Robotics** - Visual feedback for robot control

## Project Structure

```
live-vlm-webui/
‚îú‚îÄ‚îÄ server.py            # Main WebRTC server with WebSocket support
‚îú‚îÄ‚îÄ video_processor.py   # Video frame processing and VLM integration
‚îú‚îÄ‚îÄ vlm_service.py       # VLM service (OpenAI-compatible API client)
‚îú‚îÄ‚îÄ gpu_monitor.py       # Cross-platform GPU/system monitoring (NVML, etc.)
‚îú‚îÄ‚îÄ index.html           # Frontend web UI (NVIDIA-themed, dark/light mode)
‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îú‚îÄ‚îÄ start_server.sh      # Quick start script with SSL
‚îú‚îÄ‚îÄ generate_cert.sh     # SSL certificate generation script
‚îú‚îÄ‚îÄ examples.sh          # Example commands for different setups
‚îú‚îÄ‚îÄ ROADMAP.md          # Detailed future plans and milestones
‚îú‚îÄ‚îÄ .gitignore           # Git ignore patterns
‚îî‚îÄ‚îÄ README.md           # This file
```

## Troubleshooting

### Camera not accessible

**Issue:** Browser won't allow camera access
**Solution:**
- ‚úÖ Make sure you're using **HTTPS** (not HTTP)
- ‚úÖ Generate SSL certificates: `./generate_cert.sh`
- ‚úÖ Start server with SSL: `./start_server.sh` or add `--ssl-cert cert.pem --ssl-key key.pem`
- ‚úÖ Accept the security warning in your browser (Advanced ‚Üí Proceed)
- ‚úÖ Check browser permissions for camera access
- ‚úÖ Try Chrome/Edge (best WebRTC support)

**Important:** Modern browsers require HTTPS to access webcam/microphone for security reasons.

### SSL Certificate Warning

**Issue:** Browser shows "Your connection is not private" warning
**Solution:** This is normal for self-signed certificates!
1. Click **"Advanced"** or **"Show Details"**
2. Click **"Proceed to localhost (unsafe)"** or **"Accept the Risk and Continue"**
3. The warning appears because we're using a self-signed certificate for local development

For production use, get a proper SSL certificate from Let's Encrypt or a certificate authority.

### VLM connection errors
- Verify your VLM backend is running
- Check the API base URL matches your backend's port
- For vLLM: `http://localhost:8000/v1`
- For SGLang: `http://localhost:30000/v1`
- For Ollama: `http://localhost:11434/v1`

### "Model not found" errors
- Ensure the model is loaded in your backend
- Model names must match exactly
- For Ollama, use `ollama list` to see available models

### Slow performance
- Use a smaller/faster model (e.g., `llava:7b`)
- Increase `--process-every` to process fewer frames
- Reduce video resolution in `index.html`
- Ensure your VLM backend is using GPU acceleration

### Connection issues
- Check that the server is running and accessible
- Verify firewall settings if accessing from another device
- Try using `--host 0.0.0.0` to bind to all interfaces

## Development

### Customizing the VLM Service

Edit `vlm_service.py` to customize API calls:

```python
# Add custom parameters
response = await self.client.chat.completions.create(
    model=self.model,
    messages=messages,
    max_tokens=self.max_tokens,
    temperature=0.7,  # Adjust for creativity
    top_p=0.9,        # Adjust for diversity
)
```

### WebSocket Communication

The server uses WebSocket for real-time bidirectional communication:

**Server ‚Üí Client:**
- `vlm_response` - VLM analysis results and metrics
- `gpu_stats` - System monitoring data (GPU, CPU, RAM)
- `status` - Connection and processing status updates

**Client ‚Üí Server:**
- `update_prompt` - Change prompt and max_tokens on-the-fly
- `update_model` - Switch VLM model without restart
- `update_processing` - Adjust frame processing interval

Example: Sending a prompt update from JavaScript:
```javascript
websocket.send(JSON.stringify({
    type: 'update_prompt',
    prompt: 'Describe the scene',
    max_tokens: 100
}));
```

### Adding New GPU Monitors

Extend `gpu_monitor.py` for new platforms:

```python
class AppleSiliconMonitor(GPUMonitor):
    """Monitoring for Apple M1/M2/M3 chips"""

    def get_stats(self) -> Dict:
        # Use powermetrics or ioreg to get GPU stats
        # Return standardized dict format
        pass
```

### Customizing the UI Theme

Edit CSS variables in `index.html` to customize colors:

```css
:root {
    --nvidia-green: #76B900;  /* NVIDIA brand color */
    --bg-primary: #000000;    /* Dark theme background */
    --text-primary: #FFFFFF;  /* Text color */
    /* ... more variables */
}
```

## License

MIT License - Feel free to use and modify for your projects!

## Contributing

Contributions welcome! Areas for improvement:
- ‚úÖ ~~WebSocket support for dynamic prompt updates~~ (Implemented!)
- ‚úÖ ~~Live GPU/system monitoring~~ (Implemented!)
- ‚úÖ ~~Interactive prompt editor~~ (Implemented!)
- ‚úÖ ~~Inference latency metrics~~ (Implemented!)
- üîÑ Apple Silicon GPU monitoring
- üîÑ AMD GPU monitoring
- üìπ Recording functionality
- üé• Multiple simultaneous camera support
- üîä Audio description output (TTS)
- üì± Mobile app support
- üèÜ Benchmark mode with side-by-side comparison
- üìä Export analysis results (JSON, CSV)

## Acknowledgments

- Built with [aiortc](https://github.com/aiortc/aiortc) - Python WebRTC implementation
- Compatible with [vLLM](https://github.com/vllm-project/vllm), [SGLang](https://github.com/sgl-project/sglang), and [Ollama](https://ollama.ai/)
- Inspired by the growing ecosystem of open-source vision language models, including [NanoVLM](https://dusty-nv.github.io/NanoLLM/).

## Citation

If you use this in your research or project, please cite:

```bibtex
@software{live_vlm_webui,
  title = {Live VLM WebUI: Real-time Vision AI Interaction},
  year = {2025},
  url = {https://github.com/nvidia-ai-iot/live-vlm-webui}
}
```
