# Live VLM WebUI

**A universal web interface for real-time Vision Language Model interaction and benchmarking.**

Stream your webcam to any VLM and get live AI-powered analysis - perfect for testing models, benchmarking performance, and exploring vision AI capabilities across different hardware platforms.

## Key Highlights

ğŸŒ **Universal Compatibility** - Works with **any VLM served via OpenAI-compatible API** using base64-encoded images. Deploy on:
- ğŸŸ¢ **NVIDIA Jetson** (Orin, AGX Xavier)
- ğŸ”µ **NVIDIA DGX Spark** systems
- ğŸ–¥ï¸ **Desktop/Workstation** (Linux, potentially Mac)
- â˜ï¸ **Cloud APIs** (OpenAI, Anthropic, etc.)

## Features

### Core Functionality
- ğŸ¥ **Real-time WebRTC streaming** - Low-latency bidirectional video
- ğŸ”Œ **OpenAI-compatible API** - Works with vLLM, SGLang, Ollama, TGI, or any vision API endpoint that uses base64-encoded images
- ğŸ“ **Interactive prompt editor** - 10+ preset prompts (scene description, object detection, safety monitoring, OCR, etc.) + custom prompts
- âš¡ **Async processing** - Smooth video while VLM processes frames in background
- ğŸ”§ **Flexible deployment** - Local inference or cloud APIs (OpenAI, Anthropic, etc.)

### UI & Visualization
- ğŸ¨ **Modern NVIDIA-themed UI** - Professional design inspired by NVIDIA NGC Catalog
- ğŸŒ“ **Light/Dark theme toggle** - Automatic preference persistence
- ğŸ“Š **Live system monitoring** - Real-time GPU, VRAM, CPU, and RAM stats with sparkline charts
- â±ï¸ **Inference metrics** - Live latency tracking (last, average, total count)
- ğŸª **Video mirroring** - Toggle button for camera view
- ğŸ“± **Responsive layout** - Optimized for single-screen viewing (video + output + stats all visible)

### Configuration & Control
- ğŸ›ï¸ **Dynamic settings** - Change model, prompt, and processing interval without restarting
- ğŸ”„ **Model auto-detection** - Refresh button to discover available models from API
- âš™ï¸ **Adjustable processing rate** - Control frame interval (1-3600 frames, default 30)
- ğŸ¯ **Max tokens control** - Fine-tune output length (1-4096 tokens)
- ğŸ”Œ **WebSocket real-time updates** - Instant feedback on settings and analysis results

### Platform Support
- ğŸ’» **Cross-platform monitoring** - Auto-detects NVIDIA GPUs (NVML), with framework for Apple Silicon and AMD
- ğŸ–¥ï¸ **System detection** - Displays actual CPU model and hostname (Linux, macOS, Windows)
- ğŸ”’ **HTTPS support** - Self-signed certificates for secure webcam access

## Future Enhancements (Roadmap)

- [ ] Benchmark mode for side-by-side model comparison
- [ ] Model download UI (â• button placeholder ready)
- [ ] Cloud API templates (OpenAI, Anthropic quick configs)
- [ ] Recording functionality (save analysis sessions)
- [ ] Export results (JSON, CSV)
- [ ] Mobile app support

## Screenshot

![](https://github.com/user-attachments/assets/0fa02fc5-c130-43e8-b42d-acd01853270c)

## Architecture

1. **Uplink**: Webcam video â†’ WebRTC â†’ Server
2. **Processing**: Server extracts frames â†’ VLM analyzes based on your prompt (async)
3. **Downlink**: Clean video stream â†’ WebRTC â†’ Browser
4. **UI Updates**: VLM results â†’ WebSocket â†’ Real-time text display

The VLM processes frames asynchronously in the background. The video stream continues smoothly while results are displayed in a separate text panel via WebSocket, ensuring no visual interference and instant updates when new analysis completes.

## Prerequisites

- Python 3.8+
- A VLM serving backend (choose one):
  - [vLLM](https://github.com/vllm-project/vllm) (recommended for performance)
  - [SGLang](https://github.com/sgl-project/sglang) (good for complex reasoning)
  - [Ollama](https://ollama.ai/) (easiest to get started)
  - Any OpenAI-compatible API
- Webcam (V4L2 compliant video source)

## Installation

1. **Clone the repository**:
```bash
git clone https://github.com/nvidia-ai-iot/live-vlm-webui.git
cd live-vlm-webui
```

2. **Create a conda environment** (recommended):
```bash
conda create -n live-vlm-webui python=3.10 -y
conda activate live-vlm-webui
```

3. **Install Python dependencies**:
```bash
pip install -r requirements.txt
```

4. **Generate SSL certificates** (required for webcam access):
```bash
./generate_cert.sh
```

This will create `cert.pem` and `key.pem` in the project directory. These are self-signed certificates for local development.

**Note:** Modern browsers require HTTPS to access webcam/microphone. The self-signed certificate will trigger a security warning - you'll need to click "Advanced" â†’ "Proceed" to accept it.

5. **Set up your VLM backend** (choose one):

### Option A: Ollama (Easiest)
```bash
# Install ollama from https://ollama.ai/download
# Pull a vision model
ollama pull llava:7b

# Start ollama server
ollama serve
```

### Option B: vLLM (Recommended)
```bash
# Install vLLM
pip install vllm

# Start vLLM server with a vision model
python -m vllm.entrypoints.openai.api_server \
  --model llama-3.2-11b-vision-instruct \
  --port 8000
```

### Option C: SGLang
```bash
# Install SGLang
pip install "sglang[all]"

# Start SGLang server
python -m sglang.launch_server \
  --model-path llama-3.2-11b-vision-instruct \
  --port 30000
```

## Usage

### Quick Start

1. **Start your VLM backend** (see installation above)

2. **Start the server** (easiest way):
```bash
./start_server.sh
```

This will automatically start the server with SSL enabled using Ollama's `llama3.2-vision:11b` model.

3. **Open your browser** and navigate to:
```
https://<IP_ADDRESS>:8080
```

4. **Accept the security warning** (click "Advanced" â†’ "Proceed")

Click "**Advanced**" button.

![](https://github.com/user-attachments/assets/2d93e90b-708b-4834-baee-f916037b2ea1")

Then click on "**Proceeed to <IP_ADDRESS> (unsafe)**".

![](https://github.com/user-attachments/assets/455bd71f-2d87-4aa2-9da7-b75c84e8c262")

5. **Click "Start Analysis"** and allow camera access

![](https://github.com/user-attachments/assets/c2a8c58b-9271-479e-88a1-8369dbcc3178)

### Using the Web Interface

Once the server is running, the web interface provides full control:

**Left Sidebar Controls:**
- **VLM API Configuration** - Change API URL, key, and model on-the-fly
  - ğŸ”„ **Refresh Models** button to auto-detect available models
  - â• **Download Model** (coming soon)
- **Start/Stop Analysis** - Control buttons right below API config
- **Prompt Editor** - Choose from 10+ presets or write custom prompts
  - Adjust **Max Tokens** for response length (1-4096)
- **Processing Settings** - Set frame interval (1-3600 frames)
  - Lower = more frequent analysis, higher GPU usage
  - Higher = less frequent, good for benchmarking

**Main Content Area:**
- **Video Feed** - Live webcam with mirror toggle button
- **VLM Output** - Real-time results with inference metrics
- **System Stats** - Live GPU, VRAM, CPU, RAM monitoring with sparkline graphs

**Header:**
- **Connection Status** - Shows WebSocket connectivity
- **Theme Toggle** - Switch between Light/Dark modes (ğŸŒ™/â˜€ï¸)

### Manual Usage

**With vLLM**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem
```

**With SGLang**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:30000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem
```

**With Ollama**:
```bash
python server.py --model llava:7b \
  --api-base http://localhost:11434/v1 \
  --ssl-cert cert.pem --ssl-key key.pem
```

### Custom Prompts - Beyond Captioning

The real power is in custom prompts! Here are some examples:

**Scene Description (default)**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --prompt "Describe what you see in this image in one sentence."
```

**Object Detection**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --prompt "List all objects you can see in this image."
```

**Safety Monitoring**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --prompt "Alert me if you see any safety hazards or dangerous situations."
```

**Activity Recognition**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --prompt "What activity is the person performing?"
```

**Accessibility**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --prompt "Describe the scene in detail for a visually impaired person."
```

**Emotion/Expression Detection**:
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --prompt "Describe the facial expressions and emotions you observe."
```

## Configuration

### Command-line Options

```bash
python server.py --help
```

**Required**:
- `--model MODEL` - VLM model name (e.g., `llama-3.2-11b-vision-instruct`)

**Optional**:
- `--host HOST` - Host to bind to (default: `0.0.0.0`)
- `--port PORT` - Port to bind to (default: `8080`)
- `--api-base URL` - VLM API base URL (default: `http://localhost:8000/v1`)
- `--api-key KEY` - API key, use `EMPTY` for local servers (default: `EMPTY`)
- `--prompt TEXT` - Custom prompt for VLM (default: scene description)
- `--process-every N` - Process every Nth frame (default: `30`)

### Example Configurations

**High-frequency updates** (more responsive, higher CPU usage):
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --process-every 15
```

**Custom port and host**:
```bash
python server.py --model llava:7b \
  --api-base http://localhost:11434/v1 \
  --host 0.0.0.0 \
  --port 3000
```

**Using OpenAI API** (or any remote service):
```bash
python server.py --model gpt-4-vision-preview \
  --api-base https://api.openai.com/v1 \
  --api-key your-api-key-here
```

## Performance Tuning

### Frame Processing Rate
You can adjust frame processing in two ways:

**Via Command Line** (at startup):
```bash
python server.py --model llama-3.2-11b-vision-instruct \
  --api-base http://localhost:8000/v1 \
  --ssl-cert cert.pem --ssl-key key.pem \
  --process-every 60  # Process every 60 frames
```

**Via Web UI** (while running):
- Go to "Processing Settings" in the left sidebar
- Change "Frame Processing Interval" (1-3600 frames)
- Click "Apply Settings" - takes effect immediately!

**Guidelines:**
- **Lower values** (5-15 frames) = more frequent analysis, higher GPU usage (~2-6 FPS @ 30fps)
- **Default** (30 frames) = balanced, ~1 FPS analysis
- **Higher values** (60-300 frames) = less frequent, good for benchmarking (~0.1-0.5 FPS)
- **Very high** (300-3600 frames) = infrequent updates, minimal GPU load (10s-2min intervals)

### Model Selection
Choose based on your hardware and needs:

**Fast models (good for prototyping)**:
- `llava:7b` (Ollama)
- `llava-1.5-7b-hf` (vLLM/SGLang)

**Balanced**:
- `llama-3.2-11b-vision-instruct` (recommended)
- `llava:13b`

**High quality** (requires significant GPU memory):
- `llava:34b`
- `gpt-4-vision-preview` (via OpenAI API)

### Video Resolution
Edit `index.html` to change the requested video resolution:
```javascript
video: {
    width: { ideal: 640 },   // Lower for better performance
    height: { ideal: 480 }
}
```

## API Compatibility

This tool uses the OpenAI chat completions API format with vision support. Any backend that implements this standard will work:

### Tested Backends
- âœ… **vLLM** - Best performance, production-ready
- âœ… **SGLang** - Great for complex prompts
- âœ… **Ollama** - Easiest setup
- âœ… **OpenAI API** - Cloud-based (requires API key)

### Message Format
```python
{
  "model": "model-name",
  "messages": [{
    "role": "user",
    "content": [
      {"type": "text", "text": "your prompt"},
      {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}
    ]
  }]
}
```

## Use Cases

- ğŸ¬ **Content Creation** - Live scene analysis for video production
- ğŸ”’ **Security** - Real-time monitoring and alert generation
- â™¿ **Accessibility** - Visual assistance for visually impaired users
- ğŸ® **Gaming** - AI game master or interactive experiences
- ğŸ¥ **Healthcare** - Activity monitoring, fall detection
- ğŸ­ **Industrial** - Quality control, safety monitoring
- ğŸ“š **Education** - Interactive learning experiences
- ğŸ¤– **Robotics** - Visual feedback for robot control

## Project Structure

```
live-vlm-webui/
â”œâ”€â”€ server.py            # Main WebRTC server with WebSocket support
â”œâ”€â”€ video_processor.py   # Video frame processing and VLM integration
â”œâ”€â”€ vlm_service.py       # VLM service (OpenAI-compatible API client)
â”œâ”€â”€ gpu_monitor.py       # Cross-platform GPU/system monitoring (NVML, etc.)
â”œâ”€â”€ index.html           # Frontend web UI (NVIDIA-themed, dark/light mode)
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ start_server.sh      # Quick start script with SSL
â”œâ”€â”€ generate_cert.sh     # SSL certificate generation script
â”œâ”€â”€ examples.sh          # Example commands for different setups
â”œâ”€â”€ ROADMAP.md          # Detailed future plans and milestones
â”œâ”€â”€ .gitignore           # Git ignore patterns
â””â”€â”€ README.md           # This file
```

## Troubleshooting

### Camera not accessible

**Issue:** Browser won't allow camera access
**Solution:**
- âœ… Make sure you're using **HTTPS** (not HTTP)
- âœ… Generate SSL certificates: `./generate_cert.sh`
- âœ… Start server with SSL: `./start_server.sh` or add `--ssl-cert cert.pem --ssl-key key.pem`
- âœ… Accept the security warning in your browser (Advanced â†’ Proceed)
- âœ… Check browser permissions for camera access
- âœ… Try Chrome/Edge (best WebRTC support)

**Important:** Modern browsers require HTTPS to access webcam/microphone for security reasons.

### SSL Certificate Warning

**Issue:** Browser shows "Your connection is not private" warning
**Solution:** This is normal for self-signed certificates!
1. Click **"Advanced"** or **"Show Details"**
2. Click **"Proceed to localhost (unsafe)"** or **"Accept the Risk and Continue"**
3. The warning appears because we're using a self-signed certificate for local development

For production use, get a proper SSL certificate from Let's Encrypt or a certificate authority.

### VLM connection errors
- Verify your VLM backend is running
- Check the API base URL matches your backend's port
- For vLLM: `http://localhost:8000/v1`
- For SGLang: `http://localhost:30000/v1`
- For Ollama: `http://localhost:11434/v1`

### "Model not found" errors
- Ensure the model is loaded in your backend
- Model names must match exactly
- For Ollama, use `ollama list` to see available models

### Slow performance
- Use a smaller/faster model (e.g., `llava:7b`)
- Increase `--process-every` to process fewer frames
- Reduce video resolution in `index.html`
- Ensure your VLM backend is using GPU acceleration

### Connection issues
- Check that the server is running and accessible
- Verify firewall settings if accessing from another device
- Try using `--host 0.0.0.0` to bind to all interfaces

## Development

### Customizing the VLM Service

Edit `vlm_service.py` to customize API calls:

```python
# Add custom parameters
response = await self.client.chat.completions.create(
    model=self.model,
    messages=messages,
    max_tokens=self.max_tokens,
    temperature=0.7,  # Adjust for creativity
    top_p=0.9,        # Adjust for diversity
)
```

### WebSocket Communication

The server uses WebSocket for real-time bidirectional communication:

**Server â†’ Client:**
- `vlm_response` - VLM analysis results and metrics
- `gpu_stats` - System monitoring data (GPU, CPU, RAM)
- `status` - Connection and processing status updates

**Client â†’ Server:**
- `update_prompt` - Change prompt and max_tokens on-the-fly
- `update_model` - Switch VLM model without restart
- `update_processing` - Adjust frame processing interval

Example: Sending a prompt update from JavaScript:
```javascript
websocket.send(JSON.stringify({
    type: 'update_prompt',
    prompt: 'Describe the scene',
    max_tokens: 100
}));
```

### Adding New GPU Monitors

Extend `gpu_monitor.py` for new platforms:

```python
class AppleSiliconMonitor(GPUMonitor):
    """Monitoring for Apple M1/M2/M3 chips"""
    
    def get_stats(self) -> Dict:
        # Use powermetrics or ioreg to get GPU stats
        # Return standardized dict format
        pass
```

### Customizing the UI Theme

Edit CSS variables in `index.html` to customize colors:

```css
:root {
    --nvidia-green: #76B900;  /* NVIDIA brand color */
    --bg-primary: #000000;    /* Dark theme background */
    --text-primary: #FFFFFF;  /* Text color */
    /* ... more variables */
}
```

## License

MIT License - Feel free to use and modify for your projects!

## Contributing

Contributions welcome! Areas for improvement:
- âœ… ~~WebSocket support for dynamic prompt updates~~ (Implemented!)
- âœ… ~~Live GPU/system monitoring~~ (Implemented!)
- âœ… ~~Interactive prompt editor~~ (Implemented!)
- âœ… ~~Inference latency metrics~~ (Implemented!)
- ğŸ”„ Apple Silicon GPU monitoring
- ğŸ”„ AMD GPU monitoring
- ğŸ“¹ Recording functionality
- ğŸ¥ Multiple simultaneous camera support
- ğŸ”Š Audio description output (TTS)
- ğŸ“± Mobile app support
- ğŸ† Benchmark mode with side-by-side comparison
- ğŸ“Š Export analysis results (JSON, CSV)

## Acknowledgments

- Built with [aiortc](https://github.com/aiortc/aiortc) - Python WebRTC implementation
- Compatible with [vLLM](https://github.com/vllm-project/vllm), [SGLang](https://github.com/sgl-project/sglang), and [Ollama](https://ollama.ai/)
- Inspired by the growing ecosystem of open-source vision language models, including [NanoVLM](https://dusty-nv.github.io/NanoLLM/).

## Citation

If you use this in your research or project, please cite:

```bibtex
@software{live_vlm_webui,
  title = {Live VLM WebUI: Real-time Vision AI Interaction},
  year = {2025},
  url = {https://github.com/nvidia-ai-iot/live-vlm-webui}
}
```
