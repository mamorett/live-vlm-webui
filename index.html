<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Live VLM WebUI - Real-time Vision AI</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            width: 100%;
        }

        h1 {
            color: white;
            text-align: center;
            margin-bottom: 10px;
            font-size: 2.5em;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        .subtitle {
            color: rgba(255, 255, 255, 0.9);
            text-align: center;
            margin-bottom: 30px;
            font-size: 1.1em;
        }

        .video-container {
            background: white;
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
            margin-bottom: 20px;
        }

        .video-wrapper {
            position: relative;
            width: 100%;
            max-width: 640px;
            margin: 0 auto;
            border-radius: 10px;
            overflow: hidden;
            background: #000;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
        }

        video {
            width: 100%;
            height: auto;
            display: block;
        }

        .text-overlay {
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            background: rgba(0, 0, 0, 0.75);
            color: white;
            padding: 15px 20px;
            font-size: 16px;
            line-height: 1.5;
            max-height: 30%;
            overflow-y: auto;
            backdrop-filter: blur(5px);
            transition: background 0.3s ease;
        }

        .text-overlay.processing {
            background: rgba(102, 126, 234, 0.75);
        }

        .text-overlay .status {
            display: inline-block;
            font-size: 12px;
            padding: 2px 8px;
            border-radius: 3px;
            background: rgba(255, 255, 255, 0.2);
            margin-left: 10px;
            font-weight: 600;
        }

        .controls {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin-top: 25px;
            flex-wrap: wrap;
        }

        button {
            padding: 12px 30px;
            font-size: 16px;
            font-weight: 600;
            border: none;
            border-radius: 25px;
            cursor: pointer;
            transition: all 0.3s ease;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
        }

        .btn-start {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }

        .btn-start:hover:not(:disabled) {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
        }

        .btn-stop {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
        }

        .btn-stop:hover:not(:disabled) {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(245, 87, 108, 0.4);
        }

        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .status {
            text-align: center;
            margin-top: 20px;
            padding: 15px;
            border-radius: 10px;
            background: rgba(255, 255, 255, 0.9);
            font-weight: 500;
        }

        .status.connected {
            color: #27ae60;
        }

        .status.disconnected {
            color: #e74c3c;
        }

        .status.connecting {
            color: #f39c12;
        }

        .info-box {
            background: rgba(255, 255, 255, 0.95);
            border-radius: 10px;
            padding: 20px;
            margin-top: 20px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
        }

        .info-box h3 {
            color: #667eea;
            margin-bottom: 10px;
        }

        .info-box ul {
            list-style-position: inside;
            color: #555;
            line-height: 1.8;
        }

        .info-box li {
            margin-bottom: 5px;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.8em;
            }

            .video-container {
                padding: 20px;
            }

            button {
                padding: 10px 20px;
                font-size: 14px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ðŸŽ¥ Live VLM WebUI</h1>
        <p class="subtitle">Real-time vision AI interaction - describe, detect, analyze anything</p>

        <div class="video-container">
            <div class="video-wrapper">
                <video id="videoElement" autoplay playsinline muted></video>
                <div id="textOverlay" class="text-overlay">
                    <span id="vlmText">Waiting for connection...</span>
                    <span id="vlmStatus" class="status">Initializing</span>
                </div>
            </div>

            <div class="controls">
                <button id="startBtn" class="btn-start">Start VLM Analysis</button>
                <button id="stopBtn" class="btn-stop" disabled>Stop</button>
            </div>

            <div id="status" class="status disconnected">
                Click "Start VLM Analysis" to begin
            </div>
        </div>

        <div class="info-box">
            <h3>How it works:</h3>
            <ul>
                <li>Your webcam video is streamed to the server via WebRTC</li>
                <li>A Vision Language Model (VLM) analyzes frames based on your custom prompt</li>
                <li>VLM responses are overlaid on the video and streamed back in real-time</li>
                <li>Use cases: describe scenes, detect objects, monitor activities, accessibility, and more</li>
                <li>Works with any OpenAI-compatible API (vLLM, SGLang, Ollama, etc.)</li>
            </ul>
        </div>
    </div>

    <script>
        const videoElement = document.getElementById('videoElement');
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const statusElement = document.getElementById('status');
        const textOverlay = document.getElementById('textOverlay');
        const vlmText = document.getElementById('vlmText');
        const vlmStatus = document.getElementById('vlmStatus');

        let peerConnection = null;
        let localStream = null;
        let websocket = null;

        // Update status display
        function updateStatus(message, state) {
            statusElement.textContent = message;
            statusElement.className = `status ${state}`;
        }

        // Connect to WebSocket for text updates
        function connectWebSocket() {
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsUrl = `${protocol}//${window.location.host}/ws`;
            
            websocket = new WebSocket(wsUrl);
            
            websocket.onopen = () => {
                console.log('WebSocket connected');
            };
            
            websocket.onmessage = (event) => {
                const data = JSON.parse(event.data);
                
                if (data.type === 'vlm_response') {
                    // Update text overlay
                    vlmText.textContent = data.text;
                    vlmStatus.textContent = data.status;
                    
                    // Update overlay styling based on status
                    if (data.status === 'Processing...') {
                        textOverlay.classList.add('processing');
                    } else {
                        textOverlay.classList.remove('processing');
                    }
                } else if (data.type === 'status') {
                    vlmText.textContent = data.text;
                    vlmStatus.textContent = data.status;
                }
            };
            
            websocket.onerror = (error) => {
                console.error('WebSocket error:', error);
            };
            
            websocket.onclose = () => {
                console.log('WebSocket disconnected');
                vlmText.textContent = 'Disconnected from server';
                vlmStatus.textContent = 'Offline';
            };
        }

        // Disconnect WebSocket
        function disconnectWebSocket() {
            if (websocket) {
                websocket.close();
                websocket = null;
            }
        }

        // Start WebRTC connection
        async function start() {
            try {
                // Connect WebSocket for text updates
                connectWebSocket();
                
                updateStatus('Requesting camera access...', 'connecting');

                // Get local video stream
                localStream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        width: { ideal: 1280 },
                        height: { ideal: 720 }
                    },
                    audio: false
                });

                updateStatus('Connecting to server...', 'connecting');

                // Create peer connection
                peerConnection = new RTCPeerConnection({
                    iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
                });

                // Add local stream to peer connection
                localStream.getTracks().forEach(track => {
                    peerConnection.addTrack(track, localStream);
                });

                // Handle incoming stream
                peerConnection.ontrack = (event) => {
                    console.log('Received remote track');
                    if (event.streams && event.streams[0]) {
                        videoElement.srcObject = event.streams[0];
                    }
                };

                // Handle connection state changes
                peerConnection.onconnectionstatechange = () => {
                    console.log('Connection state:', peerConnection.connectionState);

                    switch(peerConnection.connectionState) {
                        case 'connected':
                            updateStatus('Connected - AI is analyzing your video!', 'connected');
                            break;
                        case 'disconnected':
                        case 'failed':
                        case 'closed':
                            updateStatus('Connection lost', 'disconnected');
                            stop();
                            break;
                    }
                };

                // Handle ICE candidates
                peerConnection.onicecandidate = (event) => {
                    // ICE candidates are logged automatically by browser devtools if needed
                    // Uncomment the line below if you want to see them in console:
                    // if (event.candidate) console.log('ICE candidate:', event.candidate);
                };

                // Create and send offer
                const offer = await peerConnection.createOffer();
                await peerConnection.setLocalDescription(offer);

                // Send offer to server
                const response = await fetch('/offer', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        sdp: peerConnection.localDescription.sdp,
                        type: peerConnection.localDescription.type
                    })
                });

                if (!response.ok) {
                    throw new Error('Failed to establish connection with server');
                }

                const answer = await response.json();

                // Set remote description
                await peerConnection.setRemoteDescription(
                    new RTCSessionDescription(answer)
                );

                // Update UI
                startBtn.disabled = true;
                stopBtn.disabled = false;

            } catch (error) {
                console.error('Error starting connection:', error);
                updateStatus(`Error: ${error.message}`, 'disconnected');
                stop();
            }
        }

        // Stop WebRTC connection
        function stop() {
            // Disconnect WebSocket
            disconnectWebSocket();
            
            // Stop local stream
            if (localStream) {
                localStream.getTracks().forEach(track => track.stop());
                localStream = null;
            }

            // Close peer connection
            if (peerConnection) {
                peerConnection.close();
                peerConnection = null;
            }

            // Clear video
            videoElement.srcObject = null;

            // Update UI
            startBtn.disabled = false;
            stopBtn.disabled = true;
            updateStatus('Stopped', 'disconnected');
            
            // Reset text overlay
            vlmText.textContent = 'Waiting for connection...';
            vlmStatus.textContent = 'Initializing';
            textOverlay.classList.remove('processing');
        }

        // Event listeners
        startBtn.addEventListener('click', start);
        stopBtn.addEventListener('click', stop);

        // Cleanup on page unload
        window.addEventListener('beforeunload', stop);
    </script>
</body>
</html>

